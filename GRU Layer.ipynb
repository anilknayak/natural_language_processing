{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones\n",
    "2. During back propagation, recurrent neural networks suffer from the vanishing gradient problem\n",
    "(vanishing gradient problem is when the gradient shrinks as it back propagates through time)\n",
    "3. Those are usually the earlier layers. So because these layers don’t learn, RNN’s can forget what it seen in longer sequences.\n",
    "4. LSTM ’s and GRU’s were created as the solution to short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial for GRU\n",
    "1. GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM\n",
    "2. GRU’s got rid of the cell state and used the hidden state to transfer information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GRU](./tutorial_imgs/1_jhi5uOm9PvZfmxvfaCektw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has two gates\n",
    "### Update Gate\n",
    "1. Update gate acts similar to the forget and input gate of an LSTM\n",
    "2. It decides what information to throw away and what new information to add.\n",
    "\n",
    "### Reset Gate\n",
    "1. Reset gate is another gate is used to decide how much past information to forget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GRU’s has fewer tensor operations\n",
    "2. They are a little speedier to train then LSTM’s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical GRU Model\n",
    "![gru_math](./tutorial_imgs/gru_math.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sigmoid function (σ) and Hyperbolic Tangent function (tanh) are used to squish the values between 0 and 1\n",
    "2. o is used as the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) or element-wise multiplication.\n",
    "3. zt functions as a filter for the previous state. If zt is low (near 0), then a lot of the previous state is reused. The input at the current state (xt) does not influence the output a lot.\n",
    "4. If zt is high, then the output at the current step is influenced a lot by the current input (xt) but it is not influenced a lot by the previous state (ht-1).\n",
    "5. rt functions as forget gate (or reset gate). It allows the cell to forget certain parts of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
